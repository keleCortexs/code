{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_123_num_cap\n",
      "param_123_num_low\n",
      "param_123_num_rus_cap\n",
      "param_123_num_eng_cap\n",
      "param_123_num_rus_low\n",
      "param_123_num_eng_low\n",
      "param_123_num_dig\n",
      "param_123_num_pun\n",
      "param_123_num_space\n",
      "param_123_num_chars\n",
      "param_123_num_words\n",
      "param_123_num_unique_words\n",
      "param_123_ratio_unique_words\n",
      "text_num_cap\n",
      "text_num_low\n",
      "text_num_rus_cap\n",
      "text_num_eng_cap\n",
      "text_num_rus_low\n",
      "text_num_eng_low\n",
      "text_num_dig\n",
      "text_num_pun\n",
      "text_num_space\n",
      "text_num_emo\n",
      "text_num_row\n",
      "text_num_chars\n",
      "text_num_words\n",
      "text_num_unique_words\n",
      "text_ratio_unique_words\n",
      "text_stopword_ratio\n",
      "text_num_stopwords\n",
      "text_num_words_upper\n",
      "text_num_words_lower\n",
      "text_num_words_title\n",
      "title_num_cap\n",
      "title_num_low\n",
      "title_num_rus_cap\n",
      "title_num_eng_cap\n",
      "title_num_rus_low\n",
      "title_num_eng_low\n",
      "title_num_dig\n",
      "title_num_pun\n",
      "title_num_space\n",
      "title_num_chars\n",
      "title_num_words\n",
      "title_num_unique_words\n",
      "title_ratio_unique_words\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import pickle\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\t\n",
    "# load data\n",
    "df_text = pickle.load(open('./data/df_text.pkl','rb'))\n",
    "\n",
    "stopwords = {x: 1 for x in stopwords.words('russian')}\n",
    "punct = set(string.punctuation)\n",
    "emoji = set()\n",
    "for s in df_text['text'].fillna('').astype(str):\n",
    "    for c in s:\n",
    "        if c.isdigit() or c.isalpha() or c.isalnum() or c.isspace() or c in punct:\n",
    "            continue\n",
    "        emoji.add(c)\n",
    "\n",
    "all = df_text.copy()\n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = ['param_123']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str) \n",
    "\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))   \n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+0.0001)\n",
    "    \n",
    "textfeats = ['text']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str)\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    all[cols + '_num_emo'] = all[cols].apply(lambda x: sum(c in emoji for c in x))\n",
    "    all[cols + '_num_row'] = all[cols].apply(lambda x: x.count('/\\n'))\n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+1) # Count Unique Words    \n",
    "    all[cols +'_stopword_ratio'] = all[cols].apply(lambda x: len([w for w in x.split() if w in stopwords])) / all[cols].apply(lambda comment: len(comment.split()))\n",
    "    all[cols +'_num_stopwords'] = all[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "    all[cols +'_num_words_upper'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    all[cols +'_num_words_lower'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    all[cols +'_num_words_title'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "textfeats = ['title']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str)\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+1)\n",
    "\n",
    "\n",
    "df_train = all[all['deal_probability'].notnull()]\n",
    "df_test = all[all['deal_probability'].isnull()]\n",
    "df_all_tmp = all.drop(['deal_probability','param_123','title','text'],axis=1)\n",
    "tmp_columns = df_all_tmp.columns.values\n",
    "for i in tmp_columns:\n",
    "    print (i)\n",
    "    df_train[i].to_pickle('./data/features/text_agg/train/' + str(i))\n",
    "    df_test[i].to_pickle('./data/features/text_agg/test/' + str(i))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
