{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "#TF_CPP_MIN_LOG_LEVEL=2\n",
    "#os.environ[\"MKL_THREADING_LAYER\"] = 'GNU'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gc\n",
    "import string\n",
    "import random\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Flatten\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, GRU, Bidirectional, Dense, Embedding, CuDNNGRU\n",
    "from keras.layers.merge import concatenate, dot, multiply, add\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam, RMSprop, adam\n",
    "from keras.layers.noise import AlphaDropout, GaussianNoise\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, MaxPooling2D, GlobalMaxPool1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "import warnings\n",
    "from AttLayer import Attention\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../feature_engineering/fe_0614_train.csv',index_col=0)\n",
    "test = pd.read_csv('../feature_engineering/fe_0614_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['aggregate'] = pd.read_pickle('../stack_sub/aggregate_train.pkl')\n",
    "test['aggregate'] = pd.read_pickle('../stack_sub/aggregate_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_train = pd.read_pickle('../image_meta_feature/aggregate3_train_exp16.pkl')\n",
    "img_test = pd.read_pickle('../image_meta_feature/aggregate3_test_exp16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = pd.DataFrame(img_train,columns='Image_'+pd.Index(range(16)).astype(str))\n",
    "img_test = pd.DataFrame(img_test,columns='Image_'+pd.Index(range(16)).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train,img_train],axis=1)\n",
    "test = pd.concat([test,img_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([train,test],ignore_index=True)\n",
    "del train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    # 'wday',\n",
    "    # 'image_top_1',\n",
    "    'param_1',\n",
    "    'param_2',\n",
    "    'param_3',\n",
    "    'city',\n",
    "    'region',\n",
    "    'category_name',\n",
    "    'parent_category_name',\n",
    "    'user_type'\n",
    "]\n",
    "\n",
    "\n",
    "remove_list = [\n",
    "    'wday',\n",
    "    'user_id',\n",
    "    'item_id',\n",
    "    'title',\n",
    "    'description',\n",
    "    'activation_date',\n",
    "    'image',\n",
    "    'deal_probability'\n",
    "]\n",
    "\n",
    "predictors = [x for x in train_data.columns if x not in remove_list]\n",
    "numerical = [x for x in predictors if x not in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical = train_data[numerical]\n",
    "\n",
    "## set threshold \n",
    "## for every numerical column, first substract the min (skip na values)\n",
    "## if max - min > threshold, take natural log of that column\n",
    "## in case log(0), add 1 after substracting min, i.e., col_val = col_val - min + 1\n",
    "## then do normalization: (col_val - min) / (max - min)\n",
    "## after normalization, impute na with mean\n",
    "\n",
    "threshold = 1000\n",
    "use_boxcox_cols = (train_data[numerical].max() -  train_data[numerical].min() > 1000).index.values\n",
    "for col in use_boxcox_cols:    \n",
    "#    train_numerical.loc[(-train_numerical[col].isnull()), col] = train_numerical.loc[(-train_numerical[col].isnull()), col] - train_numerical[col].min(skipna = True) + 1\n",
    "#    if (train_numerical[col].max(skipna = True) >= threshold):        \n",
    "    #train_numerical.loc[(-train_numerical[col].isnull()), col] = np.log1p(train_numerical.loc[(-train_numerical[col].isnull()), col])        \n",
    "    train_numerical[col] = np.log1p(train_numerical[col])\n",
    "#    train_numerical.loc[(-train_numerical[col].isnull()), col] = (train_numerical.loc[(-train_numerical[col].isnull()), col]) / (train_numerical[col].max(skipna = True))\n",
    "#    train_numerical.loc[train_numerical[col].isnull(), col] = train_numerical[col].mean()   \n",
    "sc = StandardScaler()\n",
    "train_numerical[train_numerical == np.Inf] = 0.0\n",
    "train_numerical[train_numerical == np.NINF] = 0.0\n",
    "train_numerical.fillna(0.0, inplace = True)\n",
    "train_numerical = sc.fit_transform(train_numerical)\n",
    "#train_numerical.isnull().sum()\n",
    "\n",
    "\n",
    "# In[12]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_seq(text, maxlen=100):\n",
    "    s = text.replace(',', ' ').replace('(', ' ').replace(')',' ').replace('.', ' ').strip().split()\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кокоби кокон для сна Кокон для сна малыша пользовались меньше месяца цвет серый\n"
     ]
    }
   ],
   "source": [
    "# In[14]:\n",
    "\n",
    "##================split into x_train/x_val. No stratification requried probably\n",
    "train_data['title'] = train_data.title.fillna('missing').str.lower()\n",
    "train_data['description'] = train_data.description.fillna('missing').str.lower()\n",
    "train_data['title_description'] = (train_data['title']+\" \"+train_data['description']).astype(str)\n",
    "\n",
    "train_data.title_description = train_data.title_description.map(lambda x: fill_seq(x,100))\n",
    "print train_data.title_description[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1436it [00:00, 14348.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325796it [00:26, 12453.60it/s]\n",
      "100%|██████████| 1071847/1071847 [00:01<00:00, 809453.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "EMBEDDING_FILE = '../input/fasttext.selftrained.300.model.vec'\n",
    "\n",
    "max_features = 200000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train_data['title_description']))\n",
    "\n",
    "print('getting embeddings')\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV embeddings: 0\n",
      "convert to sequences\n",
      "padding\n"
     ]
    }
   ],
   "source": [
    "print('OOV embeddings: %d' % np.mean(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print('convert to sequences')\n",
    "des_train = tokenizer.texts_to_sequences(train_data['title_description'])\n",
    "\n",
    "print('padding')\n",
    "des_train = sequence.pad_sequences(des_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ================ Create the Tokenizers\n",
    "train_data['param123'] = (train_data['param_1']+'_'+train_data['param_2']+'_'+train_data['param_3']).astype(str)\n",
    "param123_tk = {x:i+1 for i, x in enumerate(train_data.param123.unique())}\n",
    "region_tk = {x:i+1 for i, x in enumerate(train_data.region.unique())}\n",
    "city_tk =  {x:i+1 for i, x in enumerate(train_data.city.unique())}\n",
    "cat1_tk =  {x:i+1 for i, x in enumerate(train_data.parent_category_name.unique())}\n",
    "cat2_tk =  {x:i+1 for i, x in enumerate(train_data.category_name.unique())}\n",
    "param1_tk =  {x:i+1 for i, x in enumerate(train_data.param_1.unique())}\n",
    "param2_tk =  {x:i+1 for i, x in enumerate(train_data.param_2.unique())}\n",
    "param3_tk =  {x:i+1 for i, x in enumerate(train_data.param_3.unique())}\n",
    "seqnum_tk =  {x:i+1 for i, x in enumerate(train_data.item_seq_number.unique())}\n",
    "usertype_tk = {x:i+1 for i, x in enumerate(train_data.user_type.unique())}\n",
    "imgtype_tk = {x:i+1 for i, x in enumerate(train_data.image_top_1.unique())}\n",
    "tokenizers = [region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, \\\n",
    "         param3_tk, seqnum_tk, usertype_tk, imgtype_tk, param123_tk]\n",
    "\n",
    "## ================ These functions are going to get repeated on train, val, and test data\n",
    "def tokenize_data(data, tokenizers, train_numerical):\n",
    "\n",
    "    region_tk, city_tk, cat1_tk, cat2_tk, param1_tk, param2_tk, param3_tk, seqnum_tk, usertype_tk, imgtype_tk, param123_tk = tokenizers\n",
    "    x_reg = np.asarray([region_tk.get(key, 0) for key in data.region], dtype=int)\n",
    "    x_city  = np.asarray([city_tk.get(key, 0) for key in data.city], dtype=int)\n",
    "    x_cat1  = np.asarray([cat1_tk.get(key, 0) for key in data.parent_category_name], dtype=int)\n",
    "    x_cat2  = np.asarray([cat2_tk.get(key, 0) for key in data.category_name], dtype=int)\n",
    "    x_prm1 = np.asarray([param1_tk.get(key, 0) for key in data.param_1], dtype=int)\n",
    "    x_prm2 = np.asarray([param2_tk.get(key, 0) for key in data.param_2], dtype=int)\n",
    "    x_prm3 = np.asarray([param3_tk.get(key, 0) for key in data.param_3], dtype=int)\n",
    "    x_sqnm = np.asarray([seqnum_tk.get(key, 0) for key in data.item_seq_number], dtype=int)\n",
    "    x_usr = np.asarray([usertype_tk.get(key, 0) for key in data.user_type], dtype=int)\n",
    "    x_itype = np.asarray([imgtype_tk.get(key, 0) for key in data.image_top_1], dtype=int)\n",
    "    x_prm123 = np.asarray([param123_tk.get(key, 0) for key in data.param123], dtype=int)\n",
    " \n",
    "    #return [\n",
    "    #    x_reg, x_city, x_cat1, x_cat2,\n",
    "    #    x_prm1, x_prm2, x_prm3, x_sqnm,\n",
    "    #    x_usr, x_itype, dow, feat_mat\n",
    "    #]\n",
    "    return [\n",
    "        x_reg, x_city, x_cat1, x_cat2,\n",
    "        x_prm1, x_prm2, x_prm3,              #### not use x_itype, x_sqnm, \n",
    "        x_usr, x_prm123, train_numerical\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503424\n",
      "508438\n",
      "1503424\n"
     ]
    }
   ],
   "source": [
    "## ================================================================================\n",
    "#===================Final Processing on x, y train, val, test data\n",
    "x_train = tokenize_data(train_data, tokenizers, train_numerical)\n",
    "x_train.append(des_train)\n",
    "y_train = train_data.deal_probability.as_matrix()\n",
    "len_train = train_data.deal_probability.notnull().sum()\n",
    "print(len_train)\n",
    "\n",
    "## ================================================================================\n",
    "#=================== train, test, validation split\n",
    "\n",
    "x_test = [x_train[i][len_train : ] for i in range(0, len(x_train))]\n",
    "print(len(x_test[0]))\n",
    "\n",
    "x_train = [x_train[i][ : len_train] for i in range(0, len(x_train))]\n",
    "\n",
    "print(len(x_train[0]))\n",
    "#indices = np.random.permutation(len(x_train[0]))\n",
    "#print (len(indices))\n",
    "\n",
    "def get_train_val(training_idx, val_idx):\n",
    "    \n",
    "    #training_idx, val_idx = indices[ : (len_train - int(len_train * 0.1))], indices[(len_train - int(len_train * 0.1)):]\n",
    "    _xval = [x_train[i][val_idx] for i in range(0, len(x_train))]\n",
    "    _xtrain = [x_train[i][training_idx] for i in range(0, len(x_train))]\n",
    "\n",
    "    _yval = y_train[val_idx]\n",
    "    _ytrain = y_train[training_idx]\n",
    "\n",
    "    print(len(_xval[0]))\n",
    "    print(len(_xtrain[0]))\n",
    "\n",
    "    print(len(_yval))\n",
    "    print(len(_ytrain))\n",
    "    \n",
    "    return _xtrain,_ytrain,_xval,_yval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##================Beginning of the NN Model Outline.\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "drop_rate = 0.2\n",
    "emb_size = 12\n",
    "batch_size = 512\n",
    "#for drop_rate in [0, 0.1, 0.2]:\n",
    "#for batch_size in [500, 1000, 2000, 5000]:\n",
    "\n",
    "def build_model(emb_size = emb_size):\n",
    "# embedding\n",
    "    input_reg = Input(shape=(1,))\n",
    "    input_city = Input(shape=(1,))\n",
    "    input_cat1 = Input(shape=(1,))\n",
    "    input_cat2 = Input(shape=(1,))\n",
    "    input_prm1 = Input(shape=(1,))\n",
    "    input_prm2 = Input(shape=(1,))\n",
    "    input_prm3 = Input(shape=(1,))\n",
    "    #input_sqnm = Input(shape=(1,))\n",
    "    input_usr = Input(shape=(1,))\n",
    "    #input_itype = Input(shape=(1,))\n",
    "    input_prm123 = Input(shape=(1,))\n",
    "    #input_weekday = Input(shape=(1,))\n",
    "    input_hc_feat = Input(shape=(x_train[-2].shape[1],),dtype='float32')\n",
    "\n",
    "    input_des = Input(shape = (maxlen, ))\n",
    "    emb_des = Embedding(nb_words,\n",
    "                    embed_size,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = maxlen,\n",
    "                    trainable = False)(input_des)\n",
    "\n",
    "    # emb_des= SpatialDropout1D(0.2)(emb_des)\n",
    "\n",
    "    #warppers = []\n",
    "    emb_des_bigru= GRU(units=300,\n",
    "                                return_sequences = True)(emb_des)\n",
    "    emb_des_bigru = Attention(100)(emb_des_bigru)\n",
    "    #warppers.append(emb_des_bigru)\n",
    "    \n",
    "    #for kernel_size in [2,3,4]:\n",
    "    #    emb_conv = Conv1D(kernel_size=kernel_size,filters=64,padding='same')(emb_des)\n",
    "    #    emb_conv_gmp = GlobalMaxPool1D()(emb_conv)\n",
    "    #    #emb_conv_gap = GlobalAveragePooling1D()(emb_conv)\n",
    "    #    warppers.append(emb_conv_gmp)\n",
    "        #warppers.append(emb_conv_gap)\n",
    "    ## maxpooling for description features\n",
    "    #emb_des= MaxPooling1D()(emb_des)\n",
    "\n",
    "    # emb_des = Dense(64, activation=\"relu\")(emb_des)\n",
    "\n",
    "    # nsy_price = GaussianNoise(0.1)(input_price)\n",
    "\n",
    "    emb_reg  = Embedding(len(region_tk)+1, emb_size)(input_reg)\n",
    "    emb_city = Embedding(len(city_tk)+1, emb_size)(input_city)\n",
    "    emb_cat1 = Embedding(len(cat1_tk)+1, emb_size)(input_cat1)\n",
    "    emb_cat2 = Embedding(len(cat2_tk)+1, emb_size)(input_cat2)\n",
    "    emb_prm1 = Embedding(len(param1_tk)+1, emb_size)(input_prm1)\n",
    "    emb_prm2 = Embedding(len(param2_tk)+1, emb_size)(input_prm2)\n",
    "    emb_prm3 = Embedding(len(param3_tk)+1, emb_size)(input_prm3)\n",
    "    #emb_sqnm = Embedding(len(seqnum_tk)+1, emb_size)(input_sqnm)\n",
    "    emb_usr  = Embedding(len(usertype_tk)+1, emb_size)(input_usr)\n",
    "    #emb_itype= Embedding(len(imgtype_tk)+1, emb_size)(input_itype)\n",
    "    emb_prm123 = Embedding(len(param123_tk)+1, emb_size)(input_usr)\n",
    "\n",
    "    x = concatenate([Flatten() (emb_reg),\n",
    "                     Flatten() (emb_city),\n",
    "                     Flatten() (emb_cat1),\n",
    "                     Flatten() (emb_cat2),\n",
    "                     Flatten() (emb_prm1),\n",
    "                     Flatten() (emb_prm2),\n",
    "                     Flatten() (emb_prm3),\n",
    "                     #Flatten() (emb_sqnm),\n",
    "                     Flatten() (emb_usr),\n",
    "                     #Flatten() (emb_itype),\n",
    "                     Flatten() (emb_prm123),\n",
    "                     input_hc_feat,\n",
    "                     emb_des_bigru\n",
    "                     ]) # Do not want to dropout price, its noised up instead.\n",
    "\n",
    "    # x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[input_reg, input_city, input_cat1,\\\n",
    "                          input_cat2, input_prm1, input_prm2,\\\n",
    "                          input_prm3, input_usr,input_prm123,\\\n",
    "                          input_hc_feat,input_des], outputs=y)\n",
    "\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "## https://github.com/keras-team/keras/issues/1170\n",
    "# optim = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 323s 269us/step - loss: 0.2249 - val_loss: 0.2196\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 359s 298us/step - loss: 0.2194 - val_loss: 0.2190\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 326s 271us/step - loss: 0.2170 - val_loss: 0.2176\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 348s 290us/step - loss: 0.2148 - val_loss: 0.2180\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 343s 285us/step - loss: 0.2123 - val_loss: 0.2174\n",
      "Epoch 6/100\n",
      "1202739/1202739 [==============================] - 338s 281us/step - loss: 0.2093 - val_loss: 0.2190\n",
      "Epoch 7/100\n",
      "1202739/1202739 [==============================] - 342s 284us/step - loss: 0.2058 - val_loss: 0.2201\n",
      "508438/508438 [==============================] - 39s 77us/step\n",
      "300685/300685 [==============================] - 23s 75us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 367s 305us/step - loss: 0.2251 - val_loss: 0.2208\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 359s 299us/step - loss: 0.2194 - val_loss: 0.2189\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 331s 275us/step - loss: 0.2170 - val_loss: 0.2173\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 331s 275us/step - loss: 0.2147 - val_loss: 0.2169\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 326s 271us/step - loss: 0.2128 - val_loss: 0.2170\n",
      "Epoch 6/100\n",
      "1202739/1202739 [==============================] - 328s 272us/step - loss: 0.2093 - val_loss: 0.2171\n",
      "508438/508438 [==============================] - 36s 72us/step\n",
      "300685/300685 [==============================] - 21s 69us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 338s 281us/step - loss: 0.2249 - val_loss: 0.2206\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 324s 269us/step - loss: 0.2193 - val_loss: 0.2191\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 328s 272us/step - loss: 0.2170 - val_loss: 0.2179\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 322s 268us/step - loss: 0.2148 - val_loss: 0.2179\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 323s 268us/step - loss: 0.2123 - val_loss: 0.2177\n",
      "Epoch 6/100\n",
      "1202739/1202739 [==============================] - 322s 267us/step - loss: 0.2095 - val_loss: 0.2182\n",
      "Epoch 7/100\n",
      "1202739/1202739 [==============================] - 340s 283us/step - loss: 0.2060 - val_loss: 0.2212\n",
      "508438/508438 [==============================] - 39s 77us/step\n",
      "300685/300685 [==============================] - 23s 76us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 368s 306us/step - loss: 0.2252 - val_loss: 0.2206\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 357s 297us/step - loss: 0.2194 - val_loss: 0.2182\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 330s 275us/step - loss: 0.2170 - val_loss: 0.2176\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 336s 279us/step - loss: 0.2148 - val_loss: 0.2177\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 330s 275us/step - loss: 0.2125 - val_loss: 0.2169\n",
      "Epoch 6/100\n",
      "1202739/1202739 [==============================] - 350s 291us/step - loss: 0.2097 - val_loss: 0.2177\n",
      "Epoch 7/100\n",
      "1202739/1202739 [==============================] - 365s 303us/step - loss: 0.2062 - val_loss: 0.2179\n",
      "508438/508438 [==============================] - 39s 77us/step\n",
      "300685/300685 [==============================] - 23s 77us/step\n",
      "300684\n",
      "1202740\n",
      "300684\n",
      "1202740\n",
      "Train on 1202740 samples, validate on 300684 samples\n",
      "Epoch 1/100\n",
      "1202740/1202740 [==============================] - 337s 280us/step - loss: 0.2250 - val_loss: 0.2240\n",
      "Epoch 2/100\n",
      "1202740/1202740 [==============================] - 276s 230us/step - loss: 0.2195 - val_loss: 0.2201\n",
      "Epoch 3/100\n",
      "1202740/1202740 [==============================] - 270s 224us/step - loss: 0.2171 - val_loss: 0.2194\n",
      "Epoch 4/100\n",
      "1202740/1202740 [==============================] - 269s 223us/step - loss: 0.2148 - val_loss: 0.2211\n",
      "Epoch 5/100\n",
      "1202740/1202740 [==============================] - 279s 232us/step - loss: 0.2124 - val_loss: 0.2183\n",
      "Epoch 6/100\n",
      "1202740/1202740 [==============================] - 346s 287us/step - loss: 0.2095 - val_loss: 0.2190\n",
      "Epoch 7/100\n",
      "1202740/1202740 [==============================] - 344s 286us/step - loss: 0.2059 - val_loss: 0.2204\n",
      "508438/508438 [==============================] - 42s 82us/step\n",
      "300684/300684 [==============================] - 24s 79us/step\n"
     ]
    }
   ],
   "source": [
    "save_path = 'baseline_6_16_v1'\n",
    "pred_train = np.zeros((len_train,1))\n",
    "pred_test = np.zeros((len(x_test[0]),1))\n",
    "skf = KFold(len_train,n_folds=5,shuffle=True,random_state=42)\n",
    "for fold,(tr_idx,te_idx) in enumerate(skf):\n",
    "    \n",
    "        xtrain, ytrain, xval, yval = get_train_val(tr_idx, te_idx)\n",
    "        model = build_model()\n",
    "        #optim = keras.optimizers.Adam(lr=0.0005)\n",
    "        model.compile(optimizer='adam', loss=root_mean_squared_error) \n",
    "        earlystop = EarlyStopping(monitor=\"val_loss\",mode=\"auto\",\n",
    "                              patience=2,\n",
    "                          verbose=0)\n",
    "    \n",
    "        checkpt = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                              mode=\"auto\",\n",
    "                              filepath='../weights/{0}_{1}.hdf5'.format(save_path,fold),\n",
    "                              verbose=0,\n",
    "                              save_best_only=True)\n",
    "    \n",
    "        rlrop = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              mode='auto',\n",
    "                              patience=2,\n",
    "                              verbose=1,\n",
    "                              factor=0.33,\n",
    "                              cooldown=0,\n",
    "                              min_lr=1e-6)\n",
    "\n",
    "        train_history = model.fit(xtrain, ytrain,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(xval, yval),\n",
    "                              epochs=100,\n",
    "                              callbacks =[checkpt, earlystop])\n",
    "\n",
    "        model.load_weights('../weights/{0}_{1}.hdf5'.format(save_path,fold))\n",
    "        _pred_test = model.predict(x_test, batch_size=batch_size,verbose=1)\n",
    "        _pred_val = model.predict(xval, batch_size=batch_size,verbose=1)\n",
    "\n",
    "        pred_test += _pred_test.reshape((-1,1))\n",
    "        pred_train[te_idx] = _pred_val.reshape((-1,1))\n",
    "\n",
    "pred_test/=5.0\n",
    "pd.to_pickle(pred_test,'../stack_sub/{}_test.pkl'.format(save_path))\n",
    "pd.to_pickle(pred_train,'../stack_sub/{}_train.pkl'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21664113969805363"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y_train[:len_train],pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##================Beginning of the NN Model Outline.\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "drop_rate = 0.2\n",
    "emb_size = 12\n",
    "batch_size = 512\n",
    "#for drop_rate in [0, 0.1, 0.2]:\n",
    "#for batch_size in [500, 1000, 2000, 5000]:\n",
    "\n",
    "def build_model_v2(emb_size = emb_size):\n",
    "# embedding\n",
    "    input_reg = Input(shape=(1,))\n",
    "    input_city = Input(shape=(1,))\n",
    "    input_cat1 = Input(shape=(1,))\n",
    "    input_cat2 = Input(shape=(1,))\n",
    "    input_prm1 = Input(shape=(1,))\n",
    "    input_prm2 = Input(shape=(1,))\n",
    "    input_prm3 = Input(shape=(1,))\n",
    "    #input_sqnm = Input(shape=(1,))\n",
    "    input_usr = Input(shape=(1,))\n",
    "    #input_itype = Input(shape=(1,))\n",
    "    input_prm123 = Input(shape=(1,))\n",
    "    #input_weekday = Input(shape=(1,))\n",
    "    input_hc_feat = Input(shape=(x_train[-2].shape[1],),dtype='float32')\n",
    "\n",
    "    input_des = Input(shape = (maxlen, ))\n",
    "    emb_des = Embedding(nb_words,\n",
    "                    embed_size,\n",
    "                    weights = [embedding_matrix],\n",
    "                    input_length = maxlen,\n",
    "                    trainable = False)(input_des)\n",
    "\n",
    "    # emb_des= SpatialDropout1D(0.2)(emb_des)\n",
    "\n",
    "    warppers = []\n",
    "    #emb_des_bigru= Bidirectional(GRU(units=150,\n",
    "    #                            return_sequences = True))(emb_des)\n",
    "    \n",
    "    #emb_des_bigru = Attention(100)(emb_des_bigru)\n",
    "    #warppers.append(emb_des_bigru)\n",
    "    \n",
    "    for kernel_size in [2,3,4]:\n",
    "        emb_conv = Conv1D(kernel_size=kernel_size,filters=128,padding='same')(emb_des)\n",
    "        emb_conv_gmp = GlobalMaxPool1D()(emb_conv)\n",
    "    #    #emb_conv_gap = GlobalAveragePooling1D()(emb_conv)\n",
    "        warppers.append(emb_conv_gmp)\n",
    "        #warppers.append(emb_conv_gap)\n",
    "    ## maxpooling for description features\n",
    "    #emb_des= MaxPooling1D()(emb_des)\n",
    "\n",
    "    # emb_des = Dense(64, activation=\"relu\")(emb_des)\n",
    "\n",
    "    # nsy_price = GaussianNoise(0.1)(input_price)\n",
    "\n",
    "    emb_reg  = Embedding(len(region_tk)+1, emb_size)(input_reg)\n",
    "    emb_city = Embedding(len(city_tk)+1, emb_size)(input_city)\n",
    "    emb_cat1 = Embedding(len(cat1_tk)+1, emb_size)(input_cat1)\n",
    "    emb_cat2 = Embedding(len(cat2_tk)+1, emb_size)(input_cat2)\n",
    "    emb_prm1 = Embedding(len(param1_tk)+1, emb_size)(input_prm1)\n",
    "    emb_prm2 = Embedding(len(param2_tk)+1, emb_size)(input_prm2)\n",
    "    emb_prm3 = Embedding(len(param3_tk)+1, emb_size)(input_prm3)\n",
    "    #emb_sqnm = Embedding(len(seqnum_tk)+1, emb_size)(input_sqnm)\n",
    "    emb_usr  = Embedding(len(usertype_tk)+1, emb_size)(input_usr)\n",
    "    #emb_itype= Embedding(len(imgtype_tk)+1, emb_size)(input_itype)\n",
    "    emb_prm123 = Embedding(len(param123_tk)+1, emb_size)(input_usr)\n",
    "\n",
    "    x = concatenate([Flatten() (emb_reg),\n",
    "                     Flatten() (emb_city),\n",
    "                     Flatten() (emb_cat1),\n",
    "                     Flatten() (emb_cat2),\n",
    "                     Flatten() (emb_prm1),\n",
    "                     Flatten() (emb_prm2),\n",
    "                     Flatten() (emb_prm3),\n",
    "                     #Flatten() (emb_sqnm),\n",
    "                     Flatten() (emb_usr),\n",
    "                     #Flatten() (emb_itype),\n",
    "                     Flatten() (emb_prm123),\n",
    "                     input_hc_feat,\n",
    "                     #emb_des_bigru\n",
    "                     concatenate(warppers)\n",
    "                     ]) # Do not want to dropout price, its noised up instead.\n",
    "\n",
    "    # x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(drop_rate)(x)\n",
    "    y = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[input_reg, input_city, input_cat1,\\\n",
    "                          input_cat2, input_prm1, input_prm2,\\\n",
    "                          input_prm3, input_usr,input_prm123,\\\n",
    "                          input_hc_feat,input_des], outputs=y)\n",
    "\n",
    "# def rmse(y_true, y_pred):\n",
    "#     return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "## https://github.com/keras-team/keras/issues/1170\n",
    "# optim = keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 83s 69us/step - loss: 0.2244 - val_loss: 0.2202\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 82s 68us/step - loss: 0.2182 - val_loss: 0.2175\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 80s 67us/step - loss: 0.2148 - val_loss: 0.2170\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2113 - val_loss: 0.2173\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 79s 65us/step - loss: 0.2073 - val_loss: 0.2225\n",
      "508438/508438 [==============================] - 14s 27us/step\n",
      "300685/300685 [==============================] - 8s 26us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 86s 71us/step - loss: 0.2242 - val_loss: 0.2204\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2182 - val_loss: 0.2178\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2147 - val_loss: 0.2170\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2112 - val_loss: 0.2171\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 79s 65us/step - loss: 0.2070 - val_loss: 0.2197\n",
      "508438/508438 [==============================] - 14s 28us/step\n",
      "300685/300685 [==============================] - 7s 25us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 81s 67us/step - loss: 0.2243 - val_loss: 0.2193\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2182 - val_loss: 0.2181\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 80s 66us/step - loss: 0.2148 - val_loss: 0.2176\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2112 - val_loss: 0.2183\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2072 - val_loss: 0.2190\n",
      "508438/508438 [==============================] - 13s 26us/step\n",
      "300685/300685 [==============================] - 7s 25us/step\n",
      "300685\n",
      "1202739\n",
      "300685\n",
      "1202739\n",
      "Train on 1202739 samples, validate on 300685 samples\n",
      "Epoch 1/100\n",
      "1202739/1202739 [==============================] - 81s 67us/step - loss: 0.2246 - val_loss: 0.2192\n",
      "Epoch 2/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2182 - val_loss: 0.2190\n",
      "Epoch 3/100\n",
      "1202739/1202739 [==============================] - 80s 66us/step - loss: 0.2148 - val_loss: 0.2177\n",
      "Epoch 4/100\n",
      "1202739/1202739 [==============================] - 80s 67us/step - loss: 0.2112 - val_loss: 0.2184\n",
      "Epoch 5/100\n",
      "1202739/1202739 [==============================] - 79s 66us/step - loss: 0.2073 - val_loss: 0.2186\n",
      "508438/508438 [==============================] - 14s 27us/step\n",
      "300685/300685 [==============================] - 8s 25us/step\n",
      "300684\n",
      "1202740\n",
      "300684\n",
      "1202740\n",
      "Train on 1202740 samples, validate on 300684 samples\n",
      "Epoch 1/100\n",
      "1202740/1202740 [==============================] - 81s 68us/step - loss: 0.2247 - val_loss: 0.2192\n",
      "Epoch 2/100\n",
      "1202740/1202740 [==============================] - 79s 66us/step - loss: 0.2185 - val_loss: 0.2174\n",
      "Epoch 3/100\n",
      "1202740/1202740 [==============================] - 79s 66us/step - loss: 0.2150 - val_loss: 0.2172\n",
      "Epoch 4/100\n",
      "1202740/1202740 [==============================] - 80s 66us/step - loss: 0.2115 - val_loss: 0.2169\n",
      "Epoch 5/100\n",
      "1202740/1202740 [==============================] - 79s 66us/step - loss: 0.2074 - val_loss: 0.2182\n",
      "Epoch 6/100\n",
      "1202740/1202740 [==============================] - 79s 66us/step - loss: 0.2031 - val_loss: 0.2202\n",
      "508438/508438 [==============================] - 14s 27us/step\n",
      "300684/300684 [==============================] - 8s 25us/step\n"
     ]
    }
   ],
   "source": [
    "save_path = 'baseline_6_15_v4_2'\n",
    "pred_train = np.zeros((len_train,1))\n",
    "pred_test = np.zeros((len(x_test[0]),1))\n",
    "skf = KFold(len_train,n_folds=5,shuffle=True,random_state=42)\n",
    "for fold,(tr_idx,te_idx) in enumerate(skf):\n",
    "    \n",
    "        xtrain, ytrain, xval, yval = get_train_val(tr_idx, te_idx)\n",
    "        model = build_model_v2()\n",
    "        #optim = keras.optimizers.Adam(lr=0.0005)\n",
    "        model.compile(optimizer='adam', loss=root_mean_squared_error) \n",
    "        earlystop = EarlyStopping(monitor=\"val_loss\",mode=\"auto\",\n",
    "                              patience=2,\n",
    "                          verbose=0)\n",
    "    \n",
    "        checkpt = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                              mode=\"auto\",\n",
    "                              filepath='../weights/{0}_{1}.hdf5'.format(save_path,fold),\n",
    "                              verbose=0,\n",
    "                              save_best_only=True)\n",
    "    \n",
    "        rlrop = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              mode='auto',\n",
    "                              patience=2,\n",
    "                              verbose=1,\n",
    "                              factor=0.33,\n",
    "                              cooldown=0,\n",
    "                              min_lr=1e-6)\n",
    "\n",
    "        train_history = model.fit(xtrain, ytrain,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(xval, yval),\n",
    "                              epochs=100,\n",
    "                              callbacks =[checkpt, earlystop])\n",
    "\n",
    "        model.load_weights('../weights/{0}_{1}.hdf5'.format(save_path,fold))\n",
    "        _pred_test = model.predict(x_test, batch_size=batch_size,verbose=1)\n",
    "        _pred_val = model.predict(xval, batch_size=batch_size,verbose=1)\n",
    "\n",
    "        pred_test += _pred_test.reshape((-1,1))\n",
    "        pred_train[te_idx] = _pred_val.reshape((-1,1))\n",
    "\n",
    "pred_test/=5.0\n",
    "pd.to_pickle(pred_test,'../stack_sub/{}_test.pkl'.format(save_path))\n",
    "pd.to_pickle(pred_train,'../stack_sub/{}_train.pkl'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21749109694129468"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y_train[:len_train],pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
