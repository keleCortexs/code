{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates = ['activation_date'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates = ['activation_date'])\n",
    "\n",
    "train_active = pd.read_csv('../input/train_active.csv', parse_dates = ['activation_date'])\n",
    "test_active = pd.read_csv('../input/test_active.csv', parse_dates = ['activation_date'])\n",
    "train_periods = pd.read_csv('../input/periods_train.csv', parse_dates=['activation_date','date_from', 'date_to'])\n",
    "test_periods = pd.read_csv('../input/periods_test.csv', parse_dates=['activation_date','date_from', 'date_to'])\n",
    "\n",
    "df_all = pd.concat([\n",
    "    train,\n",
    "    train_active,\n",
    "    test,\n",
    "    test_active\n",
    "]).reset_index(drop=True)\n",
    "df_all.drop_duplicates(['item_id'], inplace=True)\n",
    "\n",
    "df_all['wday'] = df_all['activation_date'].dt.weekday\n",
    "\n",
    "df_all['price'].fillna(0, inplace=True)\n",
    "df_all['price'] = np.log1p(df_all['price'])\n",
    "\n",
    "df_all['city'] = df_all['city'] + \"_\" + df_all['region']\n",
    "\n",
    "df_all['param_123'] = (df_all['param_1'].fillna('') + ' ' + df_all['param_2'].fillna('') + ' ' + df_all['param_3'].fillna('')).astype(str)\n",
    "\n",
    "\n",
    "text_vars = ['user_id','region', 'city', 'parent_category_name', 'category_name', 'user_type','param_1','param_2','param_3','param_123']\n",
    "for col in tqdm(text_vars):\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(df_all[col].values.astype('str'))\n",
    "    df_all[col] = lbl.transform(df_all[col].values.astype('str'))\n",
    "    \n",
    "all_periods = pd.concat([\n",
    "    train_periods,\n",
    "    test_periods\n",
    "])\n",
    "all_periods['days_up'] = all_periods['date_to'].dt.dayofyear - all_periods['date_from'].dt.dayofyear\n",
    "all_periods['days_total'] = all_periods['date_to'].dt.dayofyear - all_periods['activation_date'].dt.dayofyear\n",
    "\n",
    "def agg(df,agg_cols):\n",
    "    for c in tqdm(agg_cols):\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(c['groupby']), c['agg'], c['target'])\n",
    "        gp = df.groupby(c['groupby'])[c['target']].agg(c['agg']).reset_index().rename(index=str, columns={c['target']:new_feature})\n",
    "        df = df.merge(gp,on=c['groupby'],how='left')\n",
    "    return df\n",
    "\n",
    "agg_cols = [\n",
    "    {'groupby': ['item_id'], 'target':'days_up', 'agg':'count'},\n",
    "    {'groupby': ['item_id'], 'target':'days_up', 'agg':'sum'},   \n",
    "]\n",
    "\n",
    "all_periods = agg(all_periods,agg_cols)\n",
    "\n",
    "all_periods.drop_duplicates(['item_id'], inplace=True)\n",
    "all_periods.drop(['activation_date','date_from','date_to','days_up','days_total'],axis=1, inplace=True)\n",
    "all_periods.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_all = df_all.merge(all_periods, on='item_id', how='left')\n",
    "\n",
    "#Impute Days up\n",
    "df_all['item_id_count_days_up_impute'] = df_all['item_id_count_days_up']\n",
    "df_all['item_id_sum_days_up_impute'] = df_all['item_id_sum_days_up']\n",
    "\n",
    "enc = df_all.groupby('category_name')['item_id_count_days_up'].agg('median').astype(np.float32).reset_index()\n",
    "enc.columns = ['category_name' ,'count_days_up_impute']\n",
    "df_all = pd.merge(df_all, enc, how='left', on='category_name')\n",
    "df_all['item_id_count_days_up_impute'].fillna(df_all['count_days_up_impute'], inplace=True)\n",
    "\n",
    "enc = df_all.groupby('category_name')['item_id_sum_days_up'].agg('median').astype(np.float32).reset_index()\n",
    "enc.columns = ['category_name' ,'sum_days_up_impute']\n",
    "df_all = pd.merge(df_all, enc, how='left', on='category_name')\n",
    "df_all['item_id_sum_days_up_impute'].fillna(df_all['sum_days_up_impute'], inplace=True)\n",
    "\n",
    "\n",
    "df_numerical_active = df_all[['category_name','city','deal_probability',\n",
    "              'item_id','item_seq_number','param_1','param_2','param_3','param_123','parent_category_name','price',\n",
    "              'region','user_id','user_type','wday','item_id_count_days_up','item_id_sum_days_up',\n",
    "              'item_id_count_days_up_impute','item_id_sum_days_up_impute']]     \n",
    "\n",
    "# create numerical features with active\n",
    "df_numerical_active.to_pickle('/tmp/basic_numerical_active.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## with active\n",
    "\n",
    "df_all_tmp = df_numerical_active.copy()\n",
    "raw_columns = df_all_tmp.columns.values\n",
    "\n",
    "## aggregate features\n",
    "def agg(df,agg_cols):\n",
    "    for c in tqdm(agg_cols):\n",
    "        new_feature = '{}_{}_{}'.format('_'.join(c['groupby']), c['agg'], c['target'])\n",
    "        gp = df.groupby(c['groupby'])[c['target']].agg(c['agg']).reset_index().rename(index=str, columns={c['target']:new_feature})\n",
    "        df = df.merge(gp,on=c['groupby'],how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "############################unique aggregation##################################\n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['parent_category_name'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['category_name'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['region'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['city'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['wday'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['param_1'], 'target':'price', 'agg':'nunique'},    \n",
    "    {'groupby': ['user_id'], 'target':'parent_category_name', 'agg':'nunique'},\n",
    "    {'groupby': ['user_id'], 'target':'category_name', 'agg':'nunique'},\n",
    "    {'groupby': ['user_id'], 'target':'wday', 'agg':'nunique'},\n",
    "    {'groupby': ['user_id'], 'target':'param_1', 'agg':'nunique'},\n",
    "    \n",
    "############################count aggregation##################################  \n",
    "    {'groupby': ['user_id'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','param_1'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','region'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','city'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','parent_category_name'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','category_name'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday','category_name'], 'target':'item_id', 'agg':'count'},    \n",
    "    {'groupby': ['user_id','wday','parent_category_name'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday','city'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday','region'], 'target':'item_id', 'agg':'count'},    \n",
    "    {'groupby': ['user_id','category_name','city'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday','category_name','city'], 'target':'item_id', 'agg':'count'},    \n",
    "    {'groupby': ['price'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['price','user_id'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['price','category_name'], 'target':'item_id', 'agg':'count'},\n",
    "     \n",
    "############################mean/median/sum/min/max aggregation##################################    \n",
    "     \n",
    "    {'groupby': ['param_2'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['param_2'], 'target':'price', 'agg':'max'},\n",
    "    {'groupby': ['param_3'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['param_3'], 'target':'price', 'agg':'max'},\n",
    "    \n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'min'},\n",
    "    {'groupby': ['user_id'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "    {'groupby': ['item_seq_number'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['item_seq_number'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['item_seq_number'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['item_seq_number'], 'target':'price', 'agg':'min'},\n",
    "    {'groupby': ['item_seq_number'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "\n",
    "    {'groupby': ['param_1'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['param_1'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "\n",
    "    {'groupby': ['region'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['region'], 'target':'price', 'agg':'max'},\n",
    "    \n",
    "    {'groupby': ['city'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['city'], 'target':'price', 'agg':'max'},\n",
    "    \n",
    "    {'groupby': ['parent_category_name'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['parent_category_name'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['parent_category_name'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "    {'groupby': ['category_name'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['category_name'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['category_name'], 'target':'price', 'agg':'max'},   \n",
    "    \n",
    "    {'groupby': ['wday','category_name','city'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['wday','category_name','city'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['wday','category_name','city'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['wday','category_name','city'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "    {'groupby': ['wday','region'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['wday','region'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['wday','region'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['wday','region'], 'target':'price', 'agg':'max'},    \n",
    "    \n",
    "    {'groupby': ['wday','city'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['wday','city'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['wday','city'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['wday','city'], 'target':'price', 'agg':'max'}, \n",
    "\n",
    "    {'groupby': ['user_id'], 'target':'item_id_sum_days_up', 'agg':'mean'},\n",
    "    {'groupby': ['user_id'], 'target':'item_id_count_days_up', 'agg':'mean'},   \n",
    "    {'groupby': ['user_id'], 'target':'item_id_sum_days_up_impute', 'agg':'mean'},\n",
    "    {'groupby': ['user_id'], 'target':'item_id_count_days_up_impute', 'agg':'mean'},    \n",
    "]\n",
    "\n",
    "df_all_tmp = agg(df_all_tmp,agg_cols)\n",
    "tmp_columns = df_all_tmp.columns.values\n",
    "\n",
    "df_train = df_all_tmp[df_all_tmp['deal_probability'].notnull()]\n",
    "test_id = pd.read_csv('../input/test.csv',usecols=['item_id'])        \n",
    "test_id = test_id.merge(df_all_tmp,on='item_id',how='left')\n",
    "del df_all_tmp\n",
    "\n",
    "for i in tmp_columns:\n",
    "    if i not in raw_columns:\n",
    "        print (i)\n",
    "        df_train[i].to_pickle('/　tmp/features/number_agg/clean_train_active/' + str(i))\n",
    "        test_id[i].to_pickle('/tmp/features/number_agg/clean_test_active/' + str(i))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates = ['activation_date'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates = ['activation_date'])\n",
    "\n",
    "df_all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "df_all['wday'] = df_all['activation_date'].dt.weekday\n",
    "\n",
    "df_all['price'].fillna(0, inplace=True)\n",
    "df_all['price'] = np.log1p(df_all['price'])\n",
    "\n",
    "df_all['city'] = df_all['city'] + \"_\" + df_all['region']\n",
    "\n",
    "df_all['param_123'] = (df_all['param_1'].fillna('') + ' ' + df_all['param_2'].fillna('') + ' ' + df_all['param_3'].fillna('')).astype(str)\n",
    "\n",
    "df_all['title'] = df_all['title'].fillna('').astype(str)\n",
    "df_all['text'] = df_all['description'].fillna('').astype(str) + ' ' + df_all['title'].fillna('').astype(str) + ' ' + df_all['param_123'].fillna('').astype(str)\n",
    "\n",
    "# from https://www.kaggle.com/christofhenkel/text2image-top-1\n",
    "# tr.csv is train_image_top_1_features.csv + test_image_top_1_features.csv\n",
    "image_top_2 = pd.read_csv('/tmp/features/category/tr.csv')\n",
    "df_all['image_top_2'] = image_top_2['image_top_1']\n",
    "\n",
    "\n",
    "text_vars = ['user_id','region', 'city', 'parent_category_name', 'category_name', 'user_type','param_1','param_2','param_3','param_123']\n",
    "for col in tqdm(text_vars):\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(df_all[col].values.astype('str'))\n",
    "    df_all[col] = lbl.transform(df_all[col].values.astype('str'))\n",
    "\n",
    "# create image_top_1,2 category feature\n",
    "df_train = df_all[df_all['deal_probability'].notnull()]\n",
    "df_test = df_all[df_all['deal_probability'].isnull()]\n",
    "\n",
    "df_train['image_top_1'].to_pickle('/tmp/features/number_agg/clean_train_image_top_1/image_top_1')\n",
    "df_test['image_top_1'].to_pickle('/tmp/features/number_agg/clean_test_image_top_1/image_top_1')  \n",
    "df_train['image_top_2'].to_pickle('/tmp/features/number_agg/clean_train_image_top_1/image_top_2')\n",
    "df_test['image_top_2'].to_pickle('/tmp/features/number_agg/clean_test_image_top_1/image_top_2')  \n",
    "\n",
    "# create image_top_1,2 aggregation feature\n",
    "df_all_tmp = df_all.copy()\n",
    "raw_columns = df_all_tmp.columns.values\n",
    "\n",
    "agg_cols = [\n",
    "\n",
    "############################unique aggregation##################################\n",
    "    \n",
    "    {'groupby': ['image_top_1'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['image_top_2'], 'target':'price', 'agg':'nunique'},\n",
    "    {'groupby': ['user_id'], 'target':'image_top_1', 'agg':'nunique'},\n",
    "    {'groupby': ['user_id'], 'target':'image_top_2', 'agg':'nunique'},\n",
    "    \n",
    "############################count aggregation##################################  \n",
    "\n",
    "    {'groupby': ['user_id','image_top_1'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','image_top_2'], 'target':'item_id', 'agg':'count'},\n",
    "\n",
    "    {'groupby': ['user_id','wday','image_top_1'], 'target':'item_id', 'agg':'count'},\n",
    "    {'groupby': ['user_id','wday','image_top_2'], 'target':'item_id', 'agg':'count'},\n",
    "\n",
    "############################mean/median/sum/min/max aggregation##################################    \n",
    "    \n",
    "    {'groupby': ['image_top_1','user_id'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['image_top_1','user_id'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['image_top_1','user_id'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['image_top_1','user_id'], 'target':'price', 'agg':'max'},\n",
    "    {'groupby': ['image_top_2','user_id'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['image_top_2','user_id'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['image_top_2','user_id'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['image_top_2','user_id'], 'target':'price', 'agg':'max'},\n",
    "\n",
    "    {'groupby': ['image_top_1'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['image_top_1'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['image_top_1'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['image_top_1'], 'target':'price', 'agg':'max'},\n",
    "    {'groupby': ['image_top_2'], 'target':'price', 'agg':'mean'},\n",
    "    {'groupby': ['image_top_2'], 'target':'price', 'agg':'median'},\n",
    "    {'groupby': ['image_top_2'], 'target':'price', 'agg':'sum'},\n",
    "    {'groupby': ['image_top_2'], 'target':'price', 'agg':'max'},\n",
    "]\n",
    "\n",
    "df_all_tmp = agg(df_all_tmp,agg_cols)\n",
    "tmp_columns = df_all_tmp.columns.values\n",
    "\n",
    "df_train = df_all_tmp[df_all_tmp['deal_probability'].notnull()]\n",
    "df_test = df_all_tmp[df_all_tmp['deal_probability'].isnull()]\n",
    "for i in tmp_columns:\n",
    "    if i not in raw_columns:\n",
    "        print (i)\n",
    "        df_train[i].to_pickle('/tmp/features/number_agg/clean_train_image_top_1/' + str(i))\n",
    "        df_test[i].to_pickle('/tmp/features/number_agg/clean_test_image_top_1/' + str(i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates = ['activation_date'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates = ['activation_date'])\n",
    "df_all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "df_all['param_123'] = (df_all['param_1'].fillna('') + ' ' + df_all['param_2'].fillna('') + ' ' + df_all['param_3'].fillna('')).astype(str)\n",
    "df_all['title'] = df_all['title'].fillna('').astype(str)\n",
    "df_all['text'] = df_all['description'].fillna('').astype(str) + ' ' + df_all['title'].fillna('').astype(str) + ' ' + df_all['param_123'].fillna('').astype(str)\n",
    "df_text = df_all[['deal_probability','title','param_123','text']]\n",
    "df_train_text = df_text[df_text['deal_probability'].notnull()]\n",
    "df_test_text = df_text[df_text['deal_probability'].isnull()]\n",
    "\n",
    "### TFIDF Vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "full_title_tfidf = tfidf_vec.fit_transform(df_text['title'].values.tolist() )\n",
    "train_title_tfidf = tfidf_vec.transform(df_train_text['title'].values.tolist())\n",
    "test_title_tfidf = tfidf_vec.transform(df_test_text['title'].values.tolist())\n",
    "\n",
    "### SVD Components ###\n",
    "n_comp = 40\n",
    "\n",
    "svd_title_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_title_obj.fit(full_title_tfidf)\n",
    "train_title_svd = pd.DataFrame(svd_title_obj.transform(train_title_tfidf))\n",
    "test_title_svd = pd.DataFrame(svd_title_obj.transform(test_title_tfidf))\n",
    "train_title_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "test_title_svd.columns = ['svd_title_'+str(i+1) for i in range(n_comp)]\n",
    "for i in train_title_svd.columns:\n",
    "    print (i)\n",
    "    test_title_svd[i].to_pickle('/tmp/features/tsvd/train/' + str(i))\n",
    "    test_title_svd[i].to_pickle('/tmp/features/tsvd/test/' + str(i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import string\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    return len(re.findall(regexp, text))\n",
    "\n",
    "stopwords = {x: 1 for x in stopwords.words('russian')}\n",
    "punct = set(string.punctuation)\n",
    "emoji = set()\n",
    "for s in df_all['text'].fillna('').astype(str):\n",
    "    for c in s:\n",
    "        if c.isdigit() or c.isalpha() or c.isalnum() or c.isspace() or c in punct:\n",
    "            continue\n",
    "        emoji.add(c)\n",
    "\n",
    "all = df_text.copy()\n",
    "\n",
    "# Meta Text Features\n",
    "textfeats = ['param_123']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str) \n",
    "\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    \n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "\n",
    "    \n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+1)\n",
    "    \n",
    "textfeats = ['text']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str)\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    \n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    all[cols + '_num_emo'] = all[cols].apply(lambda x: sum(c in emoji for c in x))\n",
    "    all[cols + '_num_row'] = all[cols].apply(lambda x: x.count('/\\n'))\n",
    "   \n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+1) # Count Unique Words    \n",
    "\n",
    "    all[cols +'_num_stopwords'] = all[cols].apply(lambda x: len([w for w in x.split() if w in stopwords]))\n",
    "    all[cols +'_num_words_upper'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    all[cols +'_num_words_lower'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "    all[cols +'_num_words_title'] = all[cols].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "  \n",
    "    \n",
    "textfeats = ['title']\n",
    "for cols in textfeats:   \n",
    "    all[cols] = all[cols].astype(str)\n",
    "    all[cols + '_num_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-ЯA-Z]', x))\n",
    "    all[cols + '_num_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-яa-z]', x))\n",
    "    all[cols + '_num_rus_cap'] = all[cols].apply(lambda x: count_regexp_occ('[А-Я]', x))\n",
    "    all[cols + '_num_eng_cap'] = all[cols].apply(lambda x: count_regexp_occ('[A-Z]', x))    \n",
    "    all[cols + '_num_rus_low'] = all[cols].apply(lambda x: count_regexp_occ('[а-я]', x))\n",
    "    all[cols + '_num_eng_low'] = all[cols].apply(lambda x: count_regexp_occ('[a-z]', x))\n",
    "    all[cols + '_num_dig'] = all[cols].apply(lambda x: count_regexp_occ('[0-9]', x))\n",
    "    \n",
    "    all[cols + '_num_pun'] = all[cols].apply(lambda x: sum(c in punct for c in x))\n",
    "    all[cols + '_num_space'] = all[cols].apply(lambda x: sum(c.isspace() for c in x))\n",
    "    \n",
    "    all[cols + '_num_chars'] = all[cols].apply(len) # Count number of Characters\n",
    "    all[cols + '_num_words'] = all[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    all[cols + '_num_unique_words'] = all[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    \n",
    "    all[cols + '_ratio_unique_words'] = all[cols+'_num_unique_words'] / (all[cols+'_num_words']+1)\n",
    "    \n",
    "df_train = all[all['deal_probability'].notnull()]\n",
    "df_test = all[all['deal_probability'].isnull()]\n",
    "df_all_tmp = all.drop(['deal_probability','param_123','title','text'],axis=1)\n",
    "tmp_columns = df_all_tmp.columns.values\n",
    "for i in tmp_columns:\n",
    "    print (i)\n",
    "    df_train[i].to_pickle('/tmp/features/text_agg/train/' + str(i))\n",
    "    df_test[i].to_pickle('/tmp/features/text_agg/test/' + str(i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords \n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from nltk.tokenize.toktok import ToktokTokenizer # tokenizer tested on russian\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk import sent_tokenize # should be multilingual\n",
    "from string import punctuation\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import FastText\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "punct = set(punctuation)\n",
    "\n",
    "# Tf-Idf\n",
    "def clean_text(s):\n",
    "    s = re.sub('м²|\\d+\\\\/\\d|\\d+-к|\\d+к', ' ', s.lower())\n",
    "    s = re.sub('\\\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "    \n",
    "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
    "russian_stop = set(stopwords.words('russian'))\n",
    "\n",
    "df_text['param_123'] = df_text['param_123'].apply(lambda x: clean_text(x))\n",
    "df_text['title'] = df_text['title'].apply(lambda x: clean_text(x))\n",
    "df_text[\"text\"] = df_text[\"text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "df_train_text = df_text[df_text['deal_probability'].notnull()]\n",
    "df_test_text = df_text[df_text['deal_probability'].isnull()]\n",
    "\n",
    "tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": 'word',\n",
    "    \"token_pattern\": r'\\w{1,}',\n",
    "    \"lowercase\": True,\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": 'l2',\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "}\n",
    "\n",
    "def get_col(col_name): return lambda x: x[col_name]\n",
    "vectorizer = FeatureUnion([\n",
    "        ('text',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=200000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col('text'))),\n",
    "        ('title',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #lowercase=True,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('title'))),\n",
    "        ('param_123',TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words = russian_stop,\n",
    "            #lowercase=True,\n",
    "            #max_features=7000,\n",
    "            preprocessor=get_col('param_123')))    \n",
    "    ])\n",
    " \n",
    "\n",
    "vectorizer.fit(df_text.to_dict('records'))\n",
    "\n",
    "ready_df_train = vectorizer.transform(df_train_text.to_dict('records'))\n",
    "ready_df_test = vectorizer.transform(df_test_text.to_dict('records'))\n",
    "\n",
    "tfvocab = vectorizer.get_feature_names()\n",
    "\n",
    "sparse.save_npz('/tmp/features/nlp/ready_df_train_200000_new.npz', ready_df_train)\n",
    "sparse.save_npz('/tmp/features/nlp/ready_df_test_200000_new.npz', ready_df_test)\n",
    "\n",
    "with open('/tmp/features/nlp/tfvocab_200000_new.pkl', 'wb') as tfvocabfile:  \n",
    "    pickle.dump(tfvocab, tfvocabfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503424 Rows and 1611702 Cols\n",
      "508438 Rows and 1611702 Cols\n",
      "Feature Names Length:  1611702\n",
      "Running fold 1 / 5\n",
      "Train Index: [      0       1       2 ... 1503420 1503421 1503423] ,Val Index: [      3       4       8 ... 1503414 1503415 1503422]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    }
   ],
   "source": [
    "##LGBM Model\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import os\n",
    "import glob\n",
    "\n",
    "df_all = pickle.load(open('/tmp/basic_numerical_active.pkl','rb'))\n",
    "df_train = df_all[df_all['deal_probability'].notnull()]\n",
    "df_test = df_all[df_all['deal_probability'].isnull()].reset_index(drop=True)\n",
    "y = df_all[df_all['deal_probability'].notnull()].deal_probability\n",
    "\n",
    "# tfidf\n",
    "ready_df_train = sparse.load_npz('/tmp/features/nlp/ready_df_train_200000_new.npz')\n",
    "ready_df_test = sparse.load_npz('/tmp/features/nlp/ready_df_test_200000_new.npz')\n",
    "tfvocab = pickle.load(open('/tmp/features/nlp/tfvocab_200000_new.pkl', 'rb'))\n",
    "\n",
    "# image - put features to /tmp/features/image/train/ /data/features/image/test/\n",
    "for fn in glob.glob('/tmp/features/image/train/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_train[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))\n",
    "    \n",
    "for fn in glob.glob('/tmp/features/image/test/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_test[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "df_train['dullnessminuswhiteness'] = df_train['dullness'] - df_train['whiteness']\n",
    "df_test['dullnessminuswhiteness'] = df_test['dullness'] - df_test['whiteness']\n",
    "\n",
    "# tsvd\n",
    "for fn in glob.glob('/tmp/features/tsvd/tmp/train/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_train[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))\n",
    "    \n",
    "for fn in glob.glob('/tmp/features/tsvd/tmp/test/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_test[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    \n",
    "# text agg\n",
    "for fn in glob.glob('/tmp/features/text_agg/train/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_train[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))\n",
    "    \n",
    "for fn in glob.glob('/tmp/features/text_agg/test/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_test[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))       \n",
    "    \n",
    "#number agg    \n",
    "for fn in glob.glob('/tmp/features/number_agg/clean_train_active/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_train[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))\n",
    "    \n",
    "for fn in glob.glob('/tmp/features/number_agg/clean_test_active/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_test[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))     \n",
    "    \n",
    "# image_top_1 image_top_2\n",
    "for fn in glob.glob('/tmp/features/number_agg/clean_train_image_top_1/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_train[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))\n",
    "    \n",
    "for fn in glob.glob('/tmp/features/number_agg/clean_test_image_top_1/*'):\n",
    "    tmp = pickle.load(open(fn,'rb')).reset_index(drop=True)\n",
    "    df_test[os.path.basename(fn)] = tmp\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "    #print (os.path.basename(fn))     \n",
    "    \n",
    "# diff features\n",
    "df_train['image_top_1_diff_price'] = df_train['price'] - df_train['image_top_1_median_price']\n",
    "df_train['parent_category_name_diff_price'] = df_train['price'] - df_train['parent_category_name_mean_price']\n",
    "df_train['category_name_diff_price'] = df_train['price'] - df_train['category_name_mean_price']\n",
    "df_train['param_1_diff_price'] = df_train['price'] - df_train['param_1_mean_price']\n",
    "df_train['param_2_diff_price'] = df_train['price'] - df_train['param_2_mean_price']\n",
    "df_train['item_seq_number_diff_price'] = df_train['price'] - df_train['item_seq_number_mean_price']\n",
    "df_train['user_id_diff_price'] = df_train['price'] - df_train['user_id_mean_price']\n",
    "df_train['region_diff_price'] = df_train['price'] - df_train['region_mean_price']\n",
    "df_train['city_diff_price'] = df_train['price'] - df_train['city_mean_price']\n",
    "\n",
    "df_test['image_top_1_diff_price'] = df_test['price'] - df_test['image_top_1_median_price']\n",
    "df_test['parent_category_name_diff_price'] = df_test['price'] - df_test['parent_category_name_mean_price']\n",
    "df_test['category_name_diff_price'] = df_test['price'] - df_test['category_name_mean_price']\n",
    "df_test['param_1_diff_price'] = df_test['price'] - df_test['param_1_mean_price']\n",
    "df_test['param_2_diff_price'] = df_test['price'] - df_test['param_2_mean_price']\n",
    "df_test['item_seq_number_diff_price'] = df_test['price'] - df_test['item_seq_number_mean_price']\n",
    "df_test['user_id_diff_price'] = df_test['price'] - df_test['user_id_mean_price']\n",
    "df_test['region_diff_price'] = df_test['price'] - df_test['region_mean_price']\n",
    "df_test['city_diff_price'] = df_test['price'] - df_test['city_mean_price']\n",
    "\n",
    "# drop_list\n",
    "drop_list = [\n",
    "    'param_123',\n",
    "    'wday_region_mean_price',\n",
    "    'wday_region_median_price',\n",
    "    'wday_region_sum_price',\n",
    "    'wday_region_max_price',   \n",
    "    'wday_city_mean_price',\n",
    "    'wday_city_median_price',\n",
    "    'wday_city_sum_price',\n",
    "    'wday_city_max_price', \n",
    "    'param_123_num_space',\n",
    "    'param_123_num_pun',\n",
    "    'title_num_pun',\n",
    "    'title_num_space',\n",
    "\n",
    " ]\n",
    "\n",
    "for d in drop_list:\n",
    "    df_train.drop([d],axis=1,inplace=True)\n",
    "    df_test.drop([d],axis=1,inplace=True)\n",
    "    \n",
    "# final feature    \n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "df_train = df_train.drop([\n",
    "                'deal_probability'],axis=1)   \n",
    "\n",
    "df_test = df_test.drop([\n",
    "                'deal_probability'],axis=1) \n",
    "\n",
    "X_tr = hstack([csr_matrix(df_train),ready_df_train]) # Sparse Matrix\n",
    "X_test = hstack([csr_matrix(df_test),ready_df_test])\n",
    "\n",
    "tfvocab = df_train.columns.tolist() + tfvocab\n",
    "\n",
    "for shape in [X_tr,X_test]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))    \n",
    "\n",
    "# train model and predict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = X_tr.tocsr()\n",
    "#del X_tra\n",
    "gc.collect()\n",
    "\n",
    "test_pred = np.zeros(X_test.shape[0])\n",
    "cat_features=['region','city','parent_category_name',\n",
    "              'category_name',\n",
    "              'user_type','image_top_1','param_1','param_2','param_3','wday']\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'xentropy',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.015,\n",
    "    'num_leaves': 600,  \n",
    "    #'max_depth': 15,  \n",
    "    'max_bin': 256,  \n",
    "    'subsample': 1,  \n",
    "    'colsample_bytree': 0.1,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "    'verbose': 1\n",
    "    }\n",
    "\n",
    "MAX_ROUNDS = 15000\n",
    "NFOLDS = 5\n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=True, random_state=228)\n",
    "xval_err = 0\n",
    "\n",
    "for i,(train_index,val_index) in enumerate(kfold.split(X,y)):\n",
    "    print(\"Running fold {} / {}\".format(i + 1, NFOLDS))\n",
    "    print(\"Train Index:\",train_index,\",Val Index:\",val_index)\n",
    "    X_tra,X_val,y_tra,y_val = X[train_index, :], X[val_index, :], y[train_index], y[val_index]\n",
    "    if i >=0:\n",
    "\n",
    "        dtrain = lgb.Dataset(\n",
    "            X_tra, label=y_tra, feature_name=tfvocab, categorical_feature=cat_features)\n",
    "        dval = lgb.Dataset(\n",
    "            X_val, label=y_val, reference=dtrain, feature_name=tfvocab, categorical_feature=cat_features)    \n",
    "        bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "            valid_sets=[dval], early_stopping_rounds=200, verbose_eval=200)\n",
    "        val_pred = bst.predict(\n",
    "            X_val, num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "        e = val_pred-y_val\n",
    "        xval_err += np.dot(e,e)\n",
    "        del dtrain,dval\n",
    "        del X_tra,y_tra,y_val,X_val\n",
    "        gc.collect()\n",
    "        test_pred_current = bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "        test_pred += bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "        test_pred_current.dump('../models/kfold5_' + str(i) + '.pkl')\n",
    "        del test_pred_current\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Full Validation RMSE:\", np.sqrt(xval_err/X.shape[0]))\n",
    "\n",
    "test_pred /= NFOLDS\n",
    "\n",
    "test = pd.read_csv('../input/test.csv', index_col = 'item_id', parse_dates = ['activation_date'])\n",
    "testdex = test.index\n",
    "sub = pd.DataFrame(test_pred,columns=[\"deal_probability\"],index=testdex)\n",
    "sub['deal_probability'] = sub['deal_probability'].clip(0.0, 1.0) # Between 0 and 1\n",
    "sub.to_csv(\"../models/sub.csv\",index=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
